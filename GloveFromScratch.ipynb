{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#essential imports\n",
    "import tensorflow as tf\n",
    "import  os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#configuration variables\n",
    "fileLocation        = \"./corpus\"\n",
    "vocab_size          = 100000\n",
    "min_occurence       = 1\n",
    "scaling_factor      = 3/4.0\n",
    "cooccurence_cap     = 100\n",
    "batch_size          = 2\n",
    "learning_rate       = 0.05\n",
    "embedding_size      = 10\n",
    "left_context_size   = 2\n",
    "right_context_size  = 2\n",
    "words               = None\n",
    "word_to_id          = None\n",
    "#cooccurence_matrix[w1, w2] = float\n",
    "cooccurence_matrix  = None\n",
    "embeddings          = None\n",
    "epoch_loss_print    = 10\n",
    "epoch_tsne_print    = 10\n",
    "log_dir             = \"./logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#utility to read the corpus\n",
    "def readCorpus(filename):\n",
    "    for line in open(filename):\n",
    "        line = line.strip()\n",
    "        yield line.lower().split()\n",
    "\n",
    "#get the left context\n",
    "def get_left_context(region, i, left_size):\n",
    "    start_index = i-left_size\n",
    "    if start_index<0:\n",
    "        start_index=0\n",
    "    left_context = region[start_index:i]\n",
    "    left_more = ['null_word']*(left_size-len(left_context))\n",
    "    left_more.extend(left_context)\n",
    "    return left_more\n",
    "\n",
    "#get the right context window\n",
    "def get_right_context(region, i, right_size):\n",
    "    end_index = i+right_size+1\n",
    "    total_region = len(region)\n",
    "    if end_index>total_region:\n",
    "        end_index=total_region\n",
    "    right_context = region[i+1:end_index]\n",
    "    right_context.extend(['null_word']*(right_size-len(right_context)))\n",
    "    return right_context\n",
    "\n",
    "#get the window\n",
    "def window(region, left_size=3, right_size=3):\n",
    "    total_region = len(region)\n",
    "    for i, word in enumerate(region):\n",
    "        left_context = get_left_context(region, i, left_size)\n",
    "        right_context = get_right_context(region, i, right_size)\n",
    "        yield (left_context, word, right_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "def fit_to_corpus(corpus, vocab_size, min_occurences, left_size, right_size):\n",
    "    words_count = Counter()\n",
    "    #provides value for non-existent key\n",
    "    cooccurence_counts = defaultdict(float)\n",
    "    for region in corpus:\n",
    "        words_count.update(region)\n",
    "        #add 1/distance from the position of centralized context word\n",
    "        for l_context, word, r_context in window(region, left_size, right_size):\n",
    "            for i, context_word in enumerate(l_context[::-1]):\n",
    "                cooccurence_counts[(word, context_word)] += 1/(i+1)\n",
    "            for i, context_word in enumerate(r_context):\n",
    "                cooccurence_counts[(word, context_word)] += 1/(i+1)\n",
    "    words = [word for word, count in words_count.most_common(max_vocab_size) if count>=min_occurences]\n",
    "    word_to_id = {word:i for i, word in enumerate(words)}\n",
    "    cooccurence_matrix = {\n",
    "                        (word_to_id[words[0]], word_to_id[words[1]]):count\n",
    "                        for words, count in cooccurence_counts.items()\n",
    "                         if words[0] in word_to_id and words[1] in word_to_id\n",
    "                        }\n",
    "    return words, word_to_id, cooccurence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the corpus\n",
    "corpus                                = readCorpus(fileLocation)\n",
    "#get words, word_to_id and cooccurence matrix by fitting it to corpus\n",
    "#we have words, wordstoid and possible cooccurence matrix for the words\n",
    "words, word_to_id, cooccurence_matrix = fit_to_corpus(corpus, max_vocab_size, min_occurence, left_context_size,\n",
    "                                                      right_context_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(14, 17): 0.0, (24, 17): 1.0, (13, 4): 0.0, (25, 12): 0.0, (24, 4): 0.0, (22, 6): 1.0, (18, 30): 0.0, (8, 5): 0.0, (5, 8): 0.0, (9, 0): 0.0, (29, 8): 1.0, (11, 5): 1.0, (16, 3): 1.0, (17, 24): 1.0, (11, 22): 0.0, (10, 18): 1.0, (24, 14): 1.0, (7, 28): 1.0, (13, 20): 1.0, (20, 4): 1.0, (23, 25): 0.0, (5, 11): 1.0, (14, 24): 1.0, (29, 11): 0.0, (30, 19): 1.0, (16, 0): 0.0, (17, 20): 0.0, (15, 1): 1.0, (18, 10): 1.0, (21, 15): 0.0, (2, 28): 1.0, (28, 15): 0.0, (14, 26): 1.0, (9, 19): 0.0, (26, 24): 0.0, (0, 27): 1.0, (1, 15): 1.0, (9, 25): 1.0, (7, 2): 0.0, (10, 30): 1.0, (7, 15): 1.0, (20, 17): 0.0, (1, 16): 0.0, (25, 0): 1.0, (3, 27): 0.0, (7, 1): 0.0, (20, 31): 0.0, (10, 19): 0.0, (0, 18): 0.0, (15, 7): 1.0, (2, 7): 0.0, (4, 17): 1.0, (15, 28): 0.0, (19, 12): 1.0, (31, 20): 0.0, (25, 9): 1.0, (26, 14): 1.0, (18, 0): 0.0, (3, 0): 1.0, (3, 21): 0.0, (31, 13): 1.0, (30, 18): 0.0, (12, 9): 1.0, (25, 23): 0.0, (21, 1): 1.0, (18, 27): 1.0, (4, 13): 0.0, (28, 2): 1.0, (27, 10): 0.0, (8, 29): 1.0, (17, 4): 1.0, (4, 24): 0.0, (3, 16): 1.0, (27, 0): 1.0, (0, 25): 1.0, (12, 25): 0.0, (22, 11): 0.0, (28, 7): 1.0, (6, 5): 0.0, (11, 8): 1.0, (15, 21): 0.0, (12, 19): 1.0, (11, 29): 0.0, (12, 30): 0.0, (1, 7): 0.0, (0, 9): 0.0, (13, 31): 1.0, (27, 3): 0.0, (9, 12): 1.0, (30, 12): 0.0, (5, 6): 0.0, (24, 26): 0.0, (0, 16): 0.0, (16, 21): 1.0, (21, 16): 1.0, (0, 3): 1.0, (16, 1): 0.0, (1, 21): 1.0, (10, 27): 0.0, (6, 22): 1.0, (5, 22): 1.0, (19, 10): 0.0, (17, 14): 0.0, (20, 13): 1.0, (8, 11): 1.0, (30, 10): 1.0, (4, 20): 1.0, (0, 23): 1.0, (19, 9): 0.0, (27, 18): 1.0, (22, 5): 1.0, (21, 3): 0.0, (23, 0): 1.0, (19, 30): 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(cooccurence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "focal_input         = tf.placeholder(tf.int32, shape=[batch_size], name=\"focal_words\")\n",
    "context_input       = tf.placeholder(tf.int32, shape=[batch_size], name=\"context_words\")\n",
    "cooccurence_count   = tf.placeholder(tf.float32, shape=[batch_size], name=\"cooccurence_count\")\n",
    "# epsilon          \n",
    "#full embedding size variables\n",
    "focal_embeddings    = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name=\"focal_embeddings\")\n",
    "context_embeddings  = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name=\"context_embeddings\")\n",
    "focal_biases        = tf.Variable(tf.random_uniform([vocab_size], -1.0, 1.0), name=\"focal_biases\")\n",
    "context_biases      = tf.Variable(tf.random_uniform([vocab_size], -1.0, 1.0), name=\"context_biases\")\n",
    "#embeddings lookup\n",
    "focal_embedding     = tf.nn.embedding_lookup([focal_embeddings], focal_input)\n",
    "context_embedding   = tf.nn.embedding_lookup([context_embeddings], context_input)\n",
    "focal_bias          = tf.nn.embedding_lookup([focal_biases], focal_input)\n",
    "context_bias        = tf.nn.embedding_lookup([context_biases], context_input)\n",
    "product             = tf.multiply(focal_embedding, context_embedding)\n",
    "embedding_product   = tf.reduce_sum(tf.multiply(focal_embedding, context_embedding), 1)\n",
    "cooccurence_epsilon = cooccurence_count+1e-10\n",
    "log_cooccurences    = tf.log(cooccurence_epsilon)\n",
    "distance_expr       = tf.square(tf.add_n([\n",
    "                        embedding_product,\n",
    "                        focal_bias,\n",
    "                        context_bias,\n",
    "                        tf.negative(log_cooccurences)]))\n",
    "count_max           = tf.constant([cooccurence_cap], dtype=tf.float32, name=\"max_cooccurence_count\")\n",
    "scaling_factor_input      = tf.constant([scaling_factor], dtype=tf.float32, name=\"scaling_factor\")\n",
    "weighting_factor    = tf.minimum(1.0, tf.pow(tf.div(cooccurence_count, count_max), scaling_factor_input))\n",
    "single_losses       = tf.multiply(weighting_factor, distance_expr)\n",
    "total_loss          = tf.reduce_sum(single_losses)\n",
    "optimizer           = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)\n",
    "combined_embeddings = tf.add(focal_embeddings, context_embeddings, name=\"combined_embeddings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchify(batch_size, *sequences):\n",
    "    for i in range(0, len(sequences[0]), batch_size):\n",
    "        yield tuple(sequence[i:i+batch_size] for sequence in sequences)\n",
    "\n",
    "def prepare_batches():\n",
    "    #get cooccurence matrix as list of elements and return each list as batch\n",
    "    cooccurrences = [(word_ids[0], word_ids[1], count) for word_ids, count in cooccurence_matrix.items()]\n",
    "    i_indices, j_indices, counts = zip(*cooccurrences)\n",
    "    return list(batchify(batch_size, i_indices, j_indices, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, path, size):\n",
    "    figure = plt.figure(figsize=size)  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        #for each label get its x and y position.\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        #text of annotation, xyposition, place label, coordinate system, \n",
    "        plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right',\n",
    "                     va='bottom')\n",
    "    if path is not None:\n",
    "        figure.savefig(path)\n",
    "        plt.close(figure)\n",
    "\n",
    "def generate_tsne(path, size=(10, 10), word_count=1000, embeddings=None):\n",
    "    #get tsne representation\n",
    "    tsne         = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "    #get the tsne transformation for each embedding\n",
    "    low_dim_embs = tsne.fit_transform(embeddings[:word_count, :])\n",
    "    #get label to assign for each point in embedding space\n",
    "    labels       = words[:word_count]\n",
    "    return plot_with_labels(low_dim_embs, labels, path, size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    #get the batches\n",
    "    total_steps=0\n",
    "    batches = prepare_batches()\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        for epoch in range(num_epochs):\n",
    "            for batch_index, batch in enumerate(batches):\n",
    "                i_s, j_s, counts = batch\n",
    "                feed_dict = {focal_input:i_s, context_input:j_s, cooccurence_count:counts}\n",
    "                sess.run([optimizer], feed_dict=feed_dict)\n",
    "                total_steps+=1\n",
    "            if epoch%epoch_loss_print==0:\n",
    "                loss = sess.run([total_loss], feed_dict)\n",
    "                print(\"Loss is \" + str(loss))\n",
    "            if epoch%epoch_tsne_print==0:\n",
    "                embeddings      = combined_embeddings.eval()\n",
    "                outputLocation  = \"\"\n",
    "                output_path = os.path.join(log_dir, \"epoch{:03d}.jpeg\".format(epoch))\n",
    "                generate_tsne(output_path, embeddings=embeddings)\n",
    "        embeddings = combined_embeddings.eval()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is [0.028760383]\n",
      "Loss is [0.0058103735]\n",
      "Loss is [0.0018895346]\n",
      "Loss is [0.00066513265]\n",
      "Loss is [0.00023938998]\n",
      "Loss is [8.7818233e-05]\n",
      "Loss is [3.2770946e-05]\n",
      "Loss is [1.2384815e-05]\n",
      "Loss is [4.7169274e-06]\n",
      "Loss is [1.8032845e-06]\n",
      "Loss is [6.8995962e-07]\n",
      "Loss is [2.636458e-07]\n",
      "Loss is [1.0046299e-07]\n",
      "Loss is [3.812718e-08]\n",
      "Loss is [1.4387706e-08]\n",
      "Loss is [5.4012039e-09]\n",
      "Loss is [2.015397e-09]\n",
      "Loss is [7.4349765e-10]\n",
      "Loss is [2.7228761e-10]\n",
      "Loss is [9.733387e-11]\n"
     ]
    }
   ],
   "source": [
    "embeddings = train(200)\n",
    "# print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image generated\n"
     ]
    }
   ],
   "source": [
    "output_path = os.path.join(log_dir, \"final_embeddings.jpeg\")\n",
    "generate_tsne(output_path, embeddings=embeddings)\n",
    "print(\"Image generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#understand how the full code works with functions written in matrix\n",
    "#refactor for printing and the corpus fully built up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plotting the t-SNE section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
